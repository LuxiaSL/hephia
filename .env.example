# API Keys for different providers
OPENPIPE_API_KEY=opk_...
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
OPENROUTER_API_KEY=sk-or-v1-...
PERPLEXITY_API_KEY=pplx-...
CHAPTER2_API_KEY=... # dummy key to work within my system

# Loop pace in seconds 
# ~2 requests per EXO_MIN_INTERVAL to COGNITIVE_MODEL, one to SUMMARY_MODEL, possibly one to VALIDATION_MODEL, of up to 7000 tokens typically.
EXO_MIN_INTERVAL = 60

# Available Models for Configuration
# OpenAI Models
# gpt4 - GPT-4 Turbo via OpenAI
# gpt3 - GPT-3.5 Turbo via OpenAI
# Anthropic Models
# newsonnet - Claude 3.6 Sonnet
# oldsonnet - Claude 3.5 Sonnet
# opus - Claude 3 Opus
# haiku - Claude 3.5 Haiku
# Google Models
# gemini - Gemini Pro
# OpenRouter Models
# mistral - Mistral 7B Instruct (Free Tier - good for validation or if you're on a budget)
# llama-70b-instruct - LLaMA 3.1 70B Instruct
# llama-405b-instruct - LLaMA 3.1 405B Instruct
# llama-405b - LLaMA 3.1 405B Base
# chapter2 - Targetted running EMS; must also configure path to desired socket if desired. 

# If you wish for more, go into config.py and follow top structure.
# If the provider doesn't exist, just ask!

# Cognitive model is the main driver
# Validation model used for processing commands fired by cognitive model
# Summary model used for cognitive contextual summaries (heavy lifting)
# Fallback model used for fallbacks when any of above fail
COGNITIVE_MODEL=newsonnet
VALIDATION_MODEL=mistral
FALLBACK_MODEL=opus
SUMMARY_MODEL=haiku

# if using chapter2 framework on unix system:
# (warning: might assume that you're running on the same machine):
# Uvicorn running on unix socket /path/to/socket
CHAPTER2_SOCKET_PATH=/path/to/socket
# fallback, or running on windows:
CHAPTER2_HTTP_PORT=8008