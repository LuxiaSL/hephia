# API Keys for different providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
DEEPSEEK_API_KEY=sk-...
OPENROUTER_API_KEY=sk-or-v1-...
PERPLEXITY_API_KEY=pplx-...
OPENPIPE_API_KEY=opk_...
CHAPTER2_API_KEY=... # dummy key to work within my system

# follow the guide in [discord_bot.md] to make a discord bot and get the token you need, if you want to enable this.
DISCORD_BOT_TOKEN=...
ENABLE_DISCORD=False

# Loop pace in seconds 
# ~2 requests per EXO_MIN_INTERVAL to COGNITIVE_MODEL, one to SUMMARY_MODEL, possibly one to VALIDATION_MODEL, of up to 7000 tokens typically.
EXO_MIN_INTERVAL = 60

# Run without the TUI/GUI
HEADLESS = False

# debugging; will make a large file.
LOG_PROMPTS = False
ADVANCED_C2_LOGGING = False

# Available Models for Configuration

# OpenAI Models
# gpt4 - GPT-4 Turbo via OpenAI
# gpt3 - GPT-3.5 Turbo via OpenAI

# Anthropic Models
# newsonnet - Claude 3.6 Sonnet
# oldsonnet - Claude 3.5 Sonnet
# opus - Claude 3 Opus
# haiku - Claude 3.5 Haiku

# Google Models
# gemini - Gemini Pro

# DeepSeek Models
# soon . . .

# OpenRouter Models
# mistral - Mistral 7B Instruct (Free Tier - good for validation or if you're on a budget)
# llama-70b-instruct - LLaMA 3.1 70B Instruct
# llama-405b-instruct - LLaMA 3.1 405B Instruct
# llama-405b - LLaMA 3.1 405B Base

# chapter2 - Targetted running Chapter 2 based EMS; must also configure path to desired socket if desired. 

# If you wish for more, go into config.py and follow top structure.
# If the provider doesn't exist, just ask!

# Cognitive model is the main driver
# Validation model used for processing commands fired by cognitive model
# Summary model used for cognitive contextual summaries (heavy lifting)
# Fallback model used for fallbacks when any of above fail
COGNITIVE_MODEL=haiku
VALIDATION_MODEL=mistral
FALLBACK_MODEL=opus
SUMMARY_MODEL=haiku

# if sentence transformers is bricking your system, you can swap to openai embedding requests.
# just make sure to include your openai api key above.
USE_LOCAL_EMBEDDING=True

# if using chapter2 framework on unix system:
# (warning: might assume that you're running on the same machine):
# Uvicorn running on unix socket /path/to/socket
CHAPTER2_SOCKET_PATH=/path/to/socket
# fallback, or running on windows:
CHAPTER2_HTTP_PORT=5519