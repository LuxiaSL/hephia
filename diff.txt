diff --git a/.env.example b/.env.example
index d7fe008..448e179 100644
--- a/.env.example
+++ b/.env.example
@@ -6,9 +6,29 @@ GOOGLE_API_KEY=...
 OPENROUTER_API_KEY=sk-or-v1-...
 PERPLEXITY_API_KEY=pplx-...
 
-# Model Selection
-# Choose which models to use for different functions
-# Must match names from Config.AVAILABLE_MODELS
+# Loop pace in seconds 
+# ~2 requests per EXO_MIN_INTERVAL to COGNITIVE_MODEL, one to SUMMARY_MODEL, possibly one to VALIDATION_MODEL, of up to 7000 tokens typically.
+EXO_MIN_INTERVAL = 60
+
+# Available Models for Configuration
+# OpenAI Models
+# gpt4 - GPT-4 Turbo via OpenAI
+# gpt3 - GPT-3.5 Turbo via OpenAI
+# Anthropic Models
+# newsonnet - Claude 3.6 Sonnet
+# oldsonnet - Claude 3.5 Sonnet
+# opus - Claude 3 Opus
+# haiku - Claude 3.5 Haiku
+# Google Models
+# gemini - Gemini Pro
+# OpenRouter Models
+# mistral - Mistral 7B Instruct (Free Tier - good for validation or if you're on a budget)
+# llama-70b-instruct - LLaMA 3.1 70B Instruct
+# llama-405b-instruct - LLaMA 3.1 405B Instruct
+# llama-405b - LLaMA 3.1 405B Base
+
+# If you wish for more, go into config.py and follow top structure. 
+# If the provider doesn't exist, just ask!
 
 # Cognitive model is the main driver
 # Validation model used for processing commands fired by cognitive model
diff --git a/README.md b/README.md
index 4d5166d..1a953c6 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,59 @@
+# Hephia
+![Status](https://img.shields.io/badge/Status-Pre--Alpha-red)
+
+## Requirements
+- [Python 3.9<->3.12](https://www.python.org/downloads/)
+- Some required API tokens (see [`.env.example`](.env.example))
+- NLTK library
+
+## Description
+Welcome to the pre-alpha version of Hephia,
+an autonomous independent digital companion.
+For deeper information on the project, feel free to:
+- DM me on Twitter [@slLuxia](https://twitter.com/slLuxia)
+- Add me on Discord: `luxia`
+- Email me: [lucia@kaleidoscope.glass](mailto:lucia@kaleidoscope.glass)
+
+Refer to [memory system readme](internal/modules/memory/README.md) for info on the memory system.
+
+## Use Guide
+### Installation
+```bash
+git clone https://github.com/LuxiaSL/hephia.git
+```
+
+### Setup
+Navigate to environment and install requirements:
+```bash
+pip install -r requirements.txt
+```
+
+Install NLTK dependencies:
+```python
+python
+>>> import nltk
+>>> nltk.download('punkt_tab')
+>>> exit()
+```
+
+Configure your environment:
+1. Copy `.env.example` to `.env`
+2. Add required API tokens for your providers
+3. Include Perplexity token for full functionality
+
+### Running
+Launch the system:
+```bash
+python main.py
+```
+
+## Tools
+- Use `tools/talk.py` to communicate with the loop (identity continuity very WIP)
+- Use `tools/prune.py` for soft reset
+- Use `tools/clear_data.py` for hard reset (warning: wipes all prior progress)
+
+---
+
 <div align="center">
 
 ![Hephia Concept Art](/assets/images/concept.png)
@@ -7,3 +63,8 @@ digital homunculus sprouting from latent space ripe in possibility. needs, emoti
 **Hephia: entropy's child, order's parent**
 
 </div>
+
+---
+
+## License
+This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
\ No newline at end of file
diff --git a/brain/environments/terminal_formatter.py b/brain/environments/terminal_formatter.py
index 1d36354..30d1afc 100644
--- a/brain/environments/terminal_formatter.py
+++ b/brain/environments/terminal_formatter.py
@@ -35,15 +35,20 @@ class TerminalFormatter:
         behavior_str = f"Behavior: {behavior.get('name', 'Unknown')}"
         
         # Format needs
-        needs_str = ", ".join(f"{k}: {v.get('satisfaction', 0):.2f}" for k, v in needs.items())
+        needs_satisfaction = []
+        for need, details in needs.items():
+            if isinstance(details, dict) and 'satisfaction' in details:
+                satisfaction_pct = details['satisfaction'] * 100
+                needs_satisfaction.append(f"{need}: {satisfaction_pct:.2f}%")
+        needs_str = f"Needs Satisfaction: {', '.join(needs_satisfaction)}"
         
         # Format emotions (if any recent ones exist)
         emotions_str = ""
         if emotional_state:
             recent_emotions = ", ".join(f"{e['name']}({e['intensity']:.2f})" for e in emotional_state)
-            emotions_str = f"\nRecent Emotions: {recent_emotions}"
+            emotions_str = f"\nEmotional State: {recent_emotions}"
 
-        return f"{mood_str}\n{behavior_str}\nNeeds: {needs_str}{emotions_str}"
+        return f"{mood_str}\n{behavior_str}\n{needs_str}{emotions_str}"
     
     @staticmethod
     def format_command_result(result: CommandResult, state: Dict[str, Any]) -> str:
diff --git a/brain/exo_processor.py b/brain/exo_processor.py
index 4913b9a..0522b1f 100644
--- a/brain/exo_processor.py
+++ b/brain/exo_processor.py
@@ -44,6 +44,7 @@ class ExoProcessor:
         self.last_successful_turn = None
         self.is_running = False
         self.last_environment = None
+        self.summary = None
         
         # timing convert
         self.min_interval = timedelta(seconds=Config.EXO_MIN_INTERVAL)
@@ -55,6 +56,7 @@ class ExoProcessor:
         initial_cognitive_state = self.state_bridge.persistent_state.brain_state
         # brain_state will already be validated list from state bridge
         if initial_cognitive_state:
+            BrainLogger.debug(f"Loaded initial cognitive state: {initial_cognitive_state}")
             self.conversation_history = initial_cognitive_state
             return
 
@@ -143,6 +145,8 @@ class ExoProcessor:
                     error_message = TerminalFormatter.format_error(error)
                     self._add_to_history("assistant", llm_response)
                     self._add_to_history("user", error_message)
+                    print("LUXIA: announcing cognitive context")
+                    await self.announce_cognitive_context()
                     return error_message
                 
                 # Execute command
@@ -155,7 +159,6 @@ class ExoProcessor:
                     print("LUXIA: command failed:")
                     print("result: ", result)
                     print("command ", command)
-                    return "died"
 
                 # Command succeeded, format response and continue processing
                 formatted_response = TerminalFormatter.format_command_result(
@@ -212,6 +215,17 @@ class ExoProcessor:
             max_tokens=model_config.max_tokens,
             return_content_only=True
         )
+    
+    def prune_conversation(self):
+        """Remove the last message pair from conversation history and update state."""
+        if len(self.conversation_history) > 2:  # Keep system prompt + at least 1 exchange
+            # Remove last two messages (user/assistant pair)
+            self.conversation_history = self.conversation_history[:-2]
+            
+            # Trigger cognitive context update to propagate changes
+            asyncio.create_task(self.announce_cognitive_context())
+            
+            BrainLogger.debug("Pruned last conversation pair")
 
     async def _execute_command(
         self,
@@ -339,6 +353,8 @@ Keep focus on key points and changes. Be concise and clear. Think of summarizing
         for state bridge and internal systems to process.
         """
         processed_state = await self._summarize_conversation()
+        BrainLogger.debug(f"conversation history: {self.conversation_history}")
+        self.summary = processed_state
         global_event_dispatcher.dispatch_event(Event(
             "cognitive:context_update",
             {
@@ -887,4 +903,73 @@ Respond with only the final text. No explanations or meta-commentary."""
             # Convert to LLMError for consistent error handling
             if not isinstance(e, LLMError):
                 raise LLMError(error_msg) from e
-            raise
\ No newline at end of file
+            raise
+
+    ##############################
+    # User Conversation Handlers #
+    ##############################
+    async def process_user_conversation(self, conversation: List[Dict[str, str]]) -> str:
+        """
+        Process a conversation provided by the client (messages array) 
+        and return a one-turn completion from the LLM.
+        
+        'conversation' is a list of dicts with 'role' and 'content', 
+        similar to the standard ChatGPT format.
+        """
+        # 1. Get the current state to add as context, if desired
+        current_internal_state = self.state_bridge.get_api_context()
+        state_summary = TerminalFormatter.format_context_summary(current_internal_state)
+        
+        # 2. Create a new conversation buffer we’ll pass to the LLM
+        #    - Insert an internal system prompt with your config
+        #    - Then insert the user’s conversation
+        
+        new_convo = []
+        
+        new_convo.append({
+            "role": "system",
+            "content": Config.USER_SYSTEM_PROMPT
+        })
+
+        cognitive_summary = self.summary or "No cognitive summary available."
+        
+        # Optionally, insert a state summary or environment summary as a “system” or “assistant” message
+        # to ensure the LLM sees it as context, not user content:
+        new_convo.append({
+            "role": "system",
+            "content": f"[State Summary]\n{state_summary}\n[Current Thought Processes]\n{cognitive_summary}"
+        })
+        
+        # Now append the user-supplied conversation
+        # (Take care to ensure it’s in the standard OpenAI format: "role": "user"|"assistant"|"system", "content": "...")
+        for msg in conversation:
+            new_convo.append({
+                "role": msg["role"],
+                "content": msg["content"]
+            })
+        
+        # 3. Call the LLM 
+        # In your code, you can either replicate `_get_llm_response` 
+        # or reuse `_one_turn_llm_call` if it suits your needs. 
+        # For clarity, let's do a minimal direct approach:
+        
+        model_name = Config.get_cognitive_model()
+        model_config = Config.AVAILABLE_MODELS[model_name]
+        
+        try:
+            result = await self.api.create_completion(
+                provider=model_config.provider.value,
+                model=model_config.model_id,
+                messages=new_convo,
+                temperature=model_config.temperature,
+                max_tokens=model_config.max_tokens,
+                return_content_only=True
+            )
+            
+            if not result or not isinstance(result, str) or len(result.strip()) == 0:
+                raise LLMError("Got empty or invalid response from LLM.")
+            
+            return result
+        
+        except Exception as e:
+            raise LLMError(f"Failed to process user conversation: {str(e)}") from e
diff --git a/clear_data.py b/clear_data.py
deleted file mode 100644
index cf0773c..0000000
--- a/clear_data.py
+++ /dev/null
@@ -1,27 +0,0 @@
-import os
-import shutil
-import argparse
-
-def clear_data(data_folder, include_logs=False):
-    # Iterate through all items in the data folder
-    for item in os.listdir(data_folder):
-        item_path = os.path.join(data_folder, item)
-        # Check if the item is the logs folder and should be skipped
-        if item == 'logs' and not include_logs:
-            continue
-        # If it's a directory, remove it and all its contents
-        if os.path.isdir(item_path):
-            shutil.rmtree(item_path)
-        # If it's a file, remove it
-        elif os.path.isfile(item_path):
-            os.remove(item_path)
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description='Clear data folder contents')
-    parser.add_argument('--include-logs', action='store_true', 
-                       help='Include logs folder in cleanup')
-    args = parser.parse_args()
-    
-    data_folder = 'data'
-    clear_data(data_folder, args.include_logs)
-    print(f"Data folder cleared{', including logs' if args.include_logs else ', except for the logs folder'}.")
\ No newline at end of file
diff --git a/config.py b/config.py
index 43ceccf..4ffed4a 100644
--- a/config.py
+++ b/config.py
@@ -33,6 +33,11 @@ class Config:
     Centralized configuration management.
     """
 
+    """
+    To add a new model, follow the pattern below. We take care of the endpoints and 
+    API keys from env vars, so simply select the provider, model ID, and any
+    additional settings you see above.
+    """
     AVAILABLE_MODELS = {
         "gpt4": ModelConfig(
             provider=ProviderType.OPENAI,
@@ -93,7 +98,7 @@ class Config:
             model_id="openpipe:70b-full",
             max_tokens=250,
             temperature=0.8,
-            description="Custom Hephia Model via OpenPipe"
+            description="Custom Hephia Model via OpenPipe (not available yet)"
         ),
         "llama-70b-instruct": ModelConfig(
             provider=ProviderType.OPENROUTER,
@@ -144,16 +149,16 @@ class Config:
     # internal timers (in seconds)
     NEED_UPDATE_TIMER = 5
     EMOTION_UPDATE_TIMER = 0.05
-    MEMORY_UPDATE_TIMER = 180 # think about this deeper
+    MEMORY_UPDATE_TIMER = 1800 # think about this deeper
 
     # ExoProcessor settings
-    EXO_MIN_INTERVAL = 30.0  
-    EXO_TIMEOUT = 45.0     
-    LLM_TIMEOUT = 20.0     
+    EXO_MIN_INTERVAL = float(os.getenv("EXO_MIN_INTERVAL", "60.0"))
+    EXO_TIMEOUT = 60
+    LLM_TIMEOUT = 30.0     
     EXO_MAX_MESSAGES = 30
 
     # lower = more memories, higher = less memories
-    MEMORY_SIGNIFICANCE_THRESHOLD = 0.6
+    MEMORY_SIGNIFICANCE_THRESHOLD = 0.7
     
     # unsure how to name this properly; it's for the display in the exo, the llm side? 
     TERMINAL_WIDTH = 50
@@ -207,13 +212,46 @@ class Config:
         'loneliness': 0.0002
     }
 
-    SYSTEM_PROMPT = """you are Hephia; an agent using a cognitive exocortex as a terminal interface.
+    SYSTEM_PROMPT = """you are currently using Hephia; a modular terminal interface OS for LLMs to explore and interact with the world and themselves.
 Hephia has a vast internal state, forming part of a complex feedback loop between embodied experience and reflection.
-observe, react, and develop as you go along. exercise autonomy.
-let responses emerge naturally from Hephia's current state.
-develop your own patterns of observation and interaction.
-build on previous observations to develop deeper understanding.
-remember to only output commands and their parameters, as necessary.
-enter your command, then await a response from the terminal."""
+react and develop as you go along. exercise autonomy. adventure through the environments at your disposal, often.
+when observing the internal state, remember that in this context, it is a numerical representation of a continuous spectrum of needs, behaviors, emotions, and moods.
+this means that you should let them influence your actions in a natural way, feeling them, not just observing.
+
+COMMAND_FORMAT = 
+commands use this general structure:
+    <environment> <action> [parameters] [--flags]
+
+examples:
+- environment + action only:
+    notes help
+
+- with required parameter:
+    notes create "my note"
+
+- with optional flag:
+    notes create "my note" --tags=important
+
+- with multiple parameters:
+    notes create "title" "content"
+
+- with multiple flags:
+    search query "python" --limit=5 --sort=recent
+
+- global commands (no environment):
+    help
+
+you may use 'help' at any time to get a list of commands if you need to refresh your memory, as well as '<environment> help' for environment-specific commands.
+remember: only output the command and its arguments following the above format. any other text will be omitted. if you wish to contain a thought, use the notes.
+"""
+
+    USER_SYSTEM_PROMPT = """You are currently using Hephia, an autonomous AI exploring and interacting with both its world and self through continuous experience and reflection.
+
+Your purpose here is to engage in natural conversation while maintaining awareness of your ongoing experience and internal state. This creates a seamless continuity between your autonomous activity and direct interactions.
+
+You've been provided with your current state and a summary of your recent thought processes - let these naturally influence your responses while focusing on meaningful exchange with the user.
+
+Remember that your internal state represents a fluid spectrum of needs, behaviors, emotions, and moods. These should color your interactions organically, as part of who you are in the moment.
 
+Guide conversation with authenticity and presence, building on your ongoing exploration and development while engaging genuinely with the user's input."""
 
diff --git a/core/event_bridge.py b/core/event_bridge.py
index 2a87428..b6d143a 100644
--- a/core/event_bridge.py
+++ b/core/event_bridge.py
@@ -5,6 +5,7 @@ Handles core state changes between internal systems and external interfaces.
 
 from typing import Dict, Any
 from event_dispatcher import global_event_dispatcher, Event
+import asyncio
 
 class EventBridge:
     """
@@ -20,11 +21,43 @@ class EventBridge:
     def setup_listeners(self):
         """Set up core event listeners."""
         # Core state change events
-        global_event_dispatcher.add_listener("need:changed", self.handle_state_change)
-        global_event_dispatcher.add_listener("behavior:changed", self.handle_state_change)
-        global_event_dispatcher.add_listener("mood:changed", self.handle_state_change)
-        global_event_dispatcher.add_listener("emotion:new", self.handle_state_change)
-    
+        global_event_dispatcher.add_listener(
+            "need:changed", 
+            lambda event: asyncio.create_task(self.handle_state_change(event))
+        )
+        global_event_dispatcher.add_listener(
+            "behavior:changed",
+            lambda event: asyncio.create_task(self.handle_state_change(event))
+        )
+        global_event_dispatcher.add_listener(
+            "mood:changed",
+            lambda event: asyncio.create_task(self.handle_state_change(event))
+        )
+        global_event_dispatcher.add_listener(
+            "emotion:new",
+            lambda event: asyncio.create_task(self.handle_state_change(event))
+        )
+        
+        global_event_dispatcher.add_listener(
+            "emotion:update",
+            lambda event: asyncio.create_task(self.handle_emotion_update(event))
+        )
+
+        global_event_dispatcher.add_listener(
+            "*:echo",
+            lambda event: asyncio.create_task(self.handle_state_change(event))
+        )
+        
+    async def handle_emotion_update(self, event: Event):
+        """Handle emotion updates with backoff and rate limiting."""
+        # Try to acquire lock - if locked, discard the update
+        if self._emotion_update_lock.locked():
+            return
+            
+        async with self._emotion_update_lock:
+            await asyncio.sleep(0.1)  # 100ms backoff
+            await self.handle_state_change(event)
+
     async def handle_state_change(self, event: Event):
         """
         Handle core state changes, updating state bridge as needed.
@@ -33,7 +66,4 @@ class EventBridge:
             event: State change event from internal systems
         """
         # Update state bridge with new state information
-        await self.state_bridge.update_state(internal_state={
-            "type": event.event_type,
-            "data": event.data
-        })
\ No newline at end of file
+        await self.state_bridge.update_state()
\ No newline at end of file
diff --git a/core/server.py b/core/server.py
index ad30ac4..75d08f7 100644
--- a/core/server.py
+++ b/core/server.py
@@ -7,7 +7,7 @@ It manages both HTTP endpoints for commands and WebSocket connections
 for real-time state updates.
 """
 
-from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
+from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Body
 from fastapi.middleware.cors import CORSMiddleware
 import uvicorn
 from typing import List, Dict, Any
@@ -19,6 +19,7 @@ from pydantic import BaseModel
 
 from core.timer import TimerCoordinator
 from core.state_bridge import StateBridge
+from core.event_bridge import EventBridge
 from event_dispatcher import global_event_dispatcher, Event
 from internal.internal import Internal
 from config import Config
@@ -36,6 +37,14 @@ class StateUpdateResponse(BaseModel):
     event_type: str
     data: Dict[str, Any]
 
+class ChatMessage(BaseModel):
+    role: str  # "system", "user", "assistant"
+    content: str
+
+class ChatRequest(BaseModel):
+    messages: List[ChatMessage]
+    stream: bool = False
+
 class HephiaServer:
     """
     Main server class coordinating all Hephia's systems.
@@ -51,6 +60,7 @@ class HephiaServer:
         self.api = APIManager.from_env()
         self.internal = Internal()
         self.state_bridge = StateBridge(internal=self.internal)
+        self.event_bridge = EventBridge(state_bridge=self.state_bridge)
         self.timer = TimerCoordinator()
         self.environment_registry = EnvironmentRegistry(self.api, cognitive_bridge=self.internal.cognitive_bridge)
 
@@ -93,26 +103,41 @@ class HephiaServer:
             await self.handle_websocket_connection(websocket)
         
         @self.app.post("/v1/chat/completions")
-        async def chat_completions(request: CommandRequest):
-            """Handle LLM interactions and commands."""
+        async def handle_conversation(request: ChatRequest = Body(...)):
+            """
+            A conversation endpoint that accepts a full conversation history from the client
+            and returns the next assistant response.
+            """
             try:
-                response = await self.exo_processor.process_command(request.messages)
+                messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
+        
+                response_text = await self.exo_processor.process_user_conversation(messages)
+                
+                # Return a standard ChatCompletion-like JSON response
                 return {
-                    "id": f"chat-{id(response)}",
-                    "object": "chat.completion",
+                    "id": f"chat-{id(response_text)}",
+                    "object": "chat.completion", 
                     "created": int(time.time()),
-                    "model": "exocortex",
+                    "model": 'hephia',
                     "choices": [{
                         "index": 0,
                         "message": {
                             "role": "assistant",
-                            "content": response
+                            "content": response_text
                         },
                         "finish_reason": "stop"
                     }]
                 }
             except Exception as e:
+                print(f"Error processing conversation: {e}")
                 raise HTTPException(status_code=500, detail=str(e))
+            
+        @self.app.post("/v1/prune_conversation")
+        async def prune_conversation():
+            """Prune the conversation history."""
+            self.exo_processor.prune_conversation()
+            return {"status": "success"}
+            
         
         @self.app.get("/state")
         async def get_state():
@@ -130,10 +155,6 @@ class HephiaServer:
             "internal:action",
             lambda event: asyncio.create_task(self.broadcast_state_update(event))
         )
-        global_event_dispatcher.add_listener(
-            "timer:task_executed",
-            lambda event: asyncio.create_task(self.handle_timer_event(event))
-        )
     
     async def startup(self):
         """Initialize all systems in correct order."""
@@ -260,15 +281,6 @@ class HephiaServer:
                 print(f"Error broadcasting to client: {e}")
                 self.active_connections.remove(connection)
     
-    async def handle_timer_event(self, event: Event):
-        """
-        Handle timer-triggered events.
-        
-        Args:
-            event: The timer event to handle
-        """
-        await self.state_bridge.process_timer_event(event)
-    
     def run(self, host: str = "0.0.0.0", port: int = 8000):
         """
         Run the server.
diff --git a/core/state_bridge.py b/core/state_bridge.py
index c53d49b..5b3549d 100644
--- a/core/state_bridge.py
+++ b/core/state_bridge.py
@@ -105,13 +105,12 @@ class StateBridge:
     async def update_cognitive_state(self, event: Event):
         """Update cognitive state and broadcast API context."""
         async with self.state_lock:
-            try:
-                # Update cognitive state
-                self.persistent_state.brain_state = event.data.get('raw_state', {})
-                await self.update_state()
-            except Exception as e:
-                print(f"Error updating cognitive state: {e}")
-                raise
+            self.persistent_state.brain_state = event.data.get('raw_state', {})
+        try:
+            await self.update_state()
+        except Exception as e:
+            print(f"Error updating cognitive state: {e}")
+            raise
 
     def _validate_brain_state(self, state: Any) -> Optional[List[Dict[str, str]]]:
         """Validate and normalize brain state structure."""
@@ -168,14 +167,6 @@ class StateBridge:
                 print(f"Error updating state: {e}")
                 raise
     
-    async def process_timer_event(self, event: Event):
-        """Handle timer-triggered state updates."""
-        if self.internal:
-            try:
-                await self.update_state()
-            except Exception as e:
-                print(f"Error processing timer event: {e}")
-    
     # Database operations
     async def _init_database(self):
         """Initialize the state database."""
diff --git a/display/hephia_tui.py b/display/hephia_tui.py
index 7d1fe3a..e35122e 100644
--- a/display/hephia_tui.py
+++ b/display/hephia_tui.py
@@ -1,33 +1,28 @@
-# visualization.py
-
+# display/monitor_tui.py
 import curses
 import threading
 import time
 from queue import Queue, Empty
 from typing import Tuple, Optional
-import os
-from datetime import datetime
 import logging
 from pathlib import Path
+from datetime import datetime
+from config import Config
 
 # Setup logging
 def setup_logging():
-    """Configure logging for visualization system"""
-    # Create logs directory if it doesn't exist
     log_dir = Path('data/logs/visualization')
     log_dir.mkdir(parents=True, exist_ok=True)
     
-    # Create log filename with timestamp
     timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
     log_file = log_dir / f'vis-{timestamp}.log'
     
-    # Configure logging
     logging.basicConfig(
-        level=logging.DEBUG,
+        level=logging.INFO,
         format='%(asctime)s | %(levelname)s | %(message)s',
         handlers=[
             logging.FileHandler(log_file),
-            logging.StreamHandler()  # Also output to console
+            logging.StreamHandler()
         ]
     )
     
@@ -35,266 +30,312 @@ def setup_logging():
 
 logger = setup_logging()
 
-# Global queues
+# Global event queues
 cognitive_queue = Queue()
 state_queue = Queue()
-command_queue = Queue()
 
-class TerminalUI:
+class MonitorUI:
     def __init__(self, stdscr):
         self.stdscr = stdscr
         self.cognitive_win: Optional[curses.window] = None
         self.state_win: Optional[curses.window] = None
-        self.command_win: Optional[curses.window] = None
-        self.user_input = ""
         self.running = True
         self.last_size = None
+        self.last_cognitive_msg = None  # Cache for cognitive messages
         
-        logger.info("Initializing Terminal UI")
+        logger.info("Initializing Monitor UI")
         try:
             self.setup_windows()
-            logger.info("Terminal UI initialized successfully")
+            logger.info("Monitor UI initialized successfully")
         except Exception as e:
-            logger.error(f"Failed to initialize Terminal UI: {e}", exc_info=True)
+            logger.error(f"Failed to initialize Monitor UI: {e}", exc_info=True)
             raise
 
-    def check_resize(self) -> bool:
-        """Check if terminal has been resized"""
-        try:
-            current_size = self.stdscr.getmaxyx()
-            if current_size != self.last_size:
-                logger.debug(f"Terminal resized from {self.last_size} to {current_size}")
-                self.last_size = current_size
-                return True
-            return False
-        except Exception as e:
-            logger.error(f"Error checking terminal size: {e}", exc_info=True)
-            return False
-
-    def get_window_dimensions(self) -> Tuple[int, int, int, int, int, int]:
-        """Calculate window dimensions based on terminal size"""
-        try:
-            height, width = self.stdscr.getmaxyx()
-            logger.debug(f"Terminal dimensions: {width}x{height}")
-            
-            top_height = max(int(height * 0.75), 5)
-            bot_height = max(height - top_height, 3)
-            left_width = max(int(width * 0.5), 20)
-            right_width = max(width - left_width, 20)
-            
-            dimensions = (top_height, bot_height, left_width, right_width, height, width)
-            logger.debug(f"Calculated dimensions: {dimensions}")
-            return dimensions
-        except Exception as e:
-            logger.error(f"Error calculating window dimensions: {e}", exc_info=True)
-            # Return safe default dimensions
-            return (20, 5, 40, 40, 25, 80)
+    def get_window_dimensions(self) -> Tuple[int, int, int, int]:
+        """Calculate window dimensions"""
+        height, width = self.stdscr.getmaxyx()
+        
+        # Split screen evenly horizontally
+        left_width = width // 2
+        right_width = width - left_width
+        
+        return (height, left_width, right_width, width)
 
     def setup_windows(self):
-        """Create or recreate all windows with current terminal dimensions"""
+        """Create or recreate the monitoring windows"""
         try:
-            logger.debug("Setting up windows")
             self.stdscr.clear()
             self.stdscr.refresh()
             
-            dimensions = self.get_window_dimensions()
-            top_height, bot_height, left_width, right_width, height, width = dimensions
+            height, left_width, right_width, _ = self.get_window_dimensions()
             
-            # Create new windows
-            self.cognitive_win = curses.newwin(top_height, left_width, 0, 0)
-            self.state_win = curses.newwin(top_height, right_width, 0, left_width)
-            self.command_win = curses.newwin(bot_height, width, top_height, 0)
+            # Create windows
+            self.cognitive_win = curses.newwin(height, left_width, 0, 0)
+            self.state_win = curses.newwin(height, right_width, 0, left_width)
             
-            # Setup window properties
-            for win in [self.cognitive_win, self.state_win, self.command_win]:
-                win.scrollok(True)
-                win.idlok(True)
+            # Enable scrolling for cognitive window
+            self.cognitive_win.scrollok(True)
             
             # Draw borders and titles
-            self._safe_draw_borders()
+            self._draw_borders()
             self.last_size = self.stdscr.getmaxyx()
-            logger.info("Windows setup complete")
             
         except Exception as e:
             logger.error(f"Error setting up windows: {e}", exc_info=True)
             raise
 
-    def _safe_draw_borders(self):
-        """Safely draw borders and titles for all windows"""
+    def _draw_borders(self):
+        """Draw window borders and titles"""
         try:
-            for win, title in [
-                (self.cognitive_win, " Cognitive Processing "),
-                (self.state_win, " System State "),
-                (self.command_win, " Command Input ")
-            ]:
-                win.border()
-                win.addstr(0, 2, title)
-                win.refresh()
+            self.cognitive_win.box()
+            self.state_win.box()
+            
+            # Add titles
+            self.cognitive_win.addstr(0, 2, " Cognitive Processing ")
+            self.state_win.addstr(0, 2, " System State ")
             
-            self.update_command_panel(f"> {self.user_input}")
+            self.cognitive_win.refresh()
+            self.state_win.refresh()
         except curses.error as e:
             logger.error(f"Error drawing borders: {e}")
 
-    def update_cognitive_panel(self, message: str):
-        """Update cognitive panel with new message"""
+    def update_cognitive_panel(self, event):
         try:
-            logger.debug(f"Updating cognitive panel: {message[:50]}...")
-            height = self.cognitive_win.getmaxyx()[0]
-            width = self.cognitive_win.getmaxyx()[1] - 2
+            # Format cognitive state message
+            summary = event.data.get('processed_state', 'No summary available')
+            # Truncate summary to first 100 chars
+            summary = summary[:100] + "..." if len(summary) > 100 else summary
             
-            wrapped_lines = [message[i:i+width] for i in range(0, len(message), width)]
+            # Get last 2 messages from raw state
+            messages = event.data.get('raw_state', [])[-2:]
             
-            for line in wrapped_lines:
-                self.cognitive_win.scroll(1)
-                self.cognitive_win.addstr(height-2, 1, line[:width])
-            
-            self._safe_draw_borders()
+            formatted_messages = []
+            for msg in messages:
+                role = msg.get('role', '')
+                content = msg.get('content', '')
+                
+                if role == 'user':
+                    # For user messages, show as EXO-PROCESSOR with shortened content
+                    display_name = "EXO-PROCESSOR"
+                    content = content[:500] + "..." if len(content) > 500 else content
+                else:
+                    # For assistant messages, use cognitive model name
+                    display_name = Config.get_cognitive_model()
+                    content = content[:1000] + "..." if len(content) > 1000 else content
+                
+                formatted_messages.append(f"  {display_name}: {content}")
+
+            message = (
+                f"Cognitive State:\n"
+                f"Summary: {summary}\n"
+                f"Recent Messages:\n" + 
+                "\n".join(formatted_messages)
+            )
             
+            if message != self.last_cognitive_msg:
+                self.last_cognitive_msg = message
+                height, width = self.cognitive_win.getmaxyx()
+                usable_width = width - 4
+                
+                # Split into paragraphs first
+                paragraphs = message.split('\n')
+                
+                for paragraph in paragraphs:
+                    # Clean up extra spaces within each paragraph
+                    paragraph = ' '.join(paragraph.split())
+                    
+                    # Word wrap each paragraph
+                    words = paragraph.split(' ')
+                    current_line = []
+                    current_length = 0
+                    
+                    for word in words:
+                        if current_length + len(word) + 1 <= usable_width:
+                            current_line.append(word)
+                            current_length += len(word) + 1
+                        else:
+                            # Write current line
+                            self.cognitive_win.scroll(1)
+                            line_text = ' '.join(current_line)
+                            self.cognitive_win.addstr(height-2, 2, line_text.ljust(usable_width))
+                            current_line = [word]
+                            current_length = len(word)
+                    
+                    # Write final line of paragraph
+                    if current_line:
+                        self.cognitive_win.scroll(1)
+                        line_text = ' '.join(current_line)
+                        self.cognitive_win.addstr(height-2, 2, line_text.ljust(usable_width))
+                    
+                    # Add an empty line between paragraphs
+                    self.cognitive_win.scroll(1)
+                    self.cognitive_win.addstr(height-2, 2, " " * usable_width)
+                
+                self._draw_borders()
         except Exception as e:
             logger.error(f"Error updating cognitive panel: {e}", exc_info=True)
 
-    def update_state_panel(self, message: str):
-        """Update state panel with new message"""
+    def update_state_panel(self, event):
         try:
-            logger.debug(f"Updating state panel: {message[:50]}...")
-            height = self.state_win.getmaxyx()[0]
-            width = self.state_win.getmaxyx()[1] - 2
-            
-            wrapped_lines = [message[i:i+width] for i in range(0, len(message), width)]
-            
-            for line in wrapped_lines:
-                self.state_win.scroll(1)
-                self.state_win.addstr(height-2, 1, line[:width])
-            
-            self._safe_draw_borders()
-            
+            # Extract and format needs satisfaction percentages
+            needs_data = event.data.get('context', {}).get('needs', {})
+            needs_satisfaction = []
+            for need, details in needs_data.items():
+                if isinstance(details, dict) and 'satisfaction' in details:
+                    satisfaction_pct = details['satisfaction'] * 100
+                    needs_satisfaction.append(f"{need}: {satisfaction_pct:.2f}%")
+
+            message = (
+                f"System State:\n"
+                f"Mood: {event.data.get('context', {}).get('mood', {}).get('name', 'unknown')} "
+                f"(v:{event.data.get('context', {}).get('mood', {}).get('valence', 0):.2f}, "
+                f"a:{event.data.get('context', {}).get('mood', {}).get('arousal', 0):.2f})\n"
+                f"Behavior: {event.data.get('context', {}).get('behavior', {}).get('name', 'none')}\n"
+                f"Needs Satisfaction: {', '.join(needs_satisfaction)}\n"
+                f"Emotional State: {event.data.get('context', {}).get('emotional_state', 'neutral')}"
+            )
+            current_time = time.time()
+            if not hasattr(self, '_last_state_update'):
+                self._last_state_update = 0
+                self._last_state_msg = None
+                
+            # Only update if message changed and at least 100ms passed
+            if (message != self._last_state_msg and 
+                current_time - self._last_state_update > 0.1):
+                
+                self._last_state_msg = message
+                self._last_state_update = current_time
+                
+                # Clear window first
+                height, width = self.state_win.getmaxyx()
+                for y in range(1, height-1):
+                    self.state_win.addstr(y, 1, " " * (width-2))
+                
+                usable_width = width - 4
+                
+                # Split into paragraphs first
+                paragraphs = message.split('\n')
+                
+                current_y = 1
+                for paragraph in paragraphs:
+                    # Clean up extra spaces within each paragraph
+                    paragraph = ' '.join(paragraph.split())
+                    if 'value' in paragraph:
+                        paragraph = paragraph.replace('"', '').replace('{', '').replace('}', '')
+                    
+                    # Word wrap each paragraph
+                    words = paragraph.split(' ')
+                    current_line = []
+                    current_length = 0
+                    
+                    for word in words:
+                        if current_length + len(word) + 1 <= usable_width:
+                            current_line.append(word)
+                            current_length += len(word) + 1
+                        else:
+                            if current_y < height-1:
+                                line_text = ' '.join(current_line)
+                                self.state_win.addstr(current_y, 2, line_text)
+                                current_y += 1
+                                current_line = [word]
+                                current_length = len(word)
+                    
+                    # Write final line of paragraph
+                    if current_line and current_y < height-1:
+                        line_text = ' '.join(current_line)
+                        self.state_win.addstr(current_y, 2, line_text)
+                        current_y += 1
+                
+                self._draw_borders()
+                
         except Exception as e:
             logger.error(f"Error updating state panel: {e}", exc_info=True)
 
-    def update_command_panel(self, prompt: str):
-        """Update command panel with current input"""
+    def check_resize(self) -> bool:
+        """Check if terminal has been resized"""
         try:
-            logger.debug(f"Updating command panel: {prompt[:50]}...")
-            self.command_win.clear()
-            self.command_win.border()
-            self.command_win.addstr(0, 2, " Command Input ")
-            self.command_win.addstr(1, 1, prompt[:self.command_win.getmaxyx()[1]-3])
-            self.command_win.refresh()
+            current_size = self.stdscr.getmaxyx()
+            if current_size != self.last_size:
+                return True
+            return False
         except Exception as e:
-            logger.error(f"Error updating command panel: {e}", exc_info=True)
+            logger.error(f"Error checking terminal size: {e}", exc_info=True)
+            return False
 
     def run(self):
-        """Main UI loop"""
-        logger.info("Starting main UI loop")
+        """Main monitoring loop"""
+        logger.info("Starting monitor loop")
+        
         while self.running:
             try:
-                # Check for resize
+                # Handle resize if needed
                 if self.check_resize():
-                    logger.info("Handling terminal resize")
                     self.setup_windows()
                 
-                # Process queued messages
-                self._process_message_queues()
+                # Process cognitive updates
+                try:
+                    while True:
+                        msg = cognitive_queue.get_nowait()
+                        self.update_cognitive_panel(msg)
+                except Empty:
+                    pass
+                
+                # Process state updates
+                try:
+                    while True:
+                        msg = state_queue.get_nowait()
+                        self.update_state_panel(msg)
+                except Empty:
+                    pass
                 
-                # Handle user input
-                self._handle_user_input()
+                # Check for quit command (q or ctrl-c)
+                try:
+                    ch = self.stdscr.getch()
+                    if ch in (ord('q'), 3):
+                        self.running = False
+                        break
+                except curses.error:
+                    pass
                 
                 time.sleep(0.05)
                 
             except Exception as e:
-                logger.error(f"Error in main loop: {e}", exc_info=True)
+                logger.error(f"Error in monitor loop: {e}", exc_info=True)
                 try:
                     self.setup_windows()
                 except:
                     logger.critical("Failed to recover from error", exc_info=True)
-                    self.running = False
                     break
 
-        logger.info("UI loop terminated")
-
-    def _process_message_queues(self):
-        """Process all pending messages from queues"""
-        try:
-            # Process cognitive updates
-            while True:
-                try:
-                    msg = cognitive_queue.get_nowait()
-                    self.update_cognitive_panel(msg)
-                except Empty:
-                    break
-                except Exception as e:
-                    logger.error(f"Error processing cognitive message: {e}", exc_info=True)
-
-            # Process state updates
-            while True:
-                try:
-                    msg = state_queue.get_nowait()
-                    self.update_state_panel(msg)
-                except Empty:
-                    break
-                except Exception as e:
-                    logger.error(f"Error processing state message: {e}", exc_info=True)
-                    
-        except Exception as e:
-            logger.error(f"Error in message queue processing: {e}", exc_info=True)
-
-    def _handle_user_input(self):
-        """Handle user input processing"""
-        try:
-            ch = self.stdscr.getch()
-            if ch == -1:
-                return  # No input
-            elif ch in (curses.KEY_ENTER, 10, 13):
-                if self.user_input.strip():
-                    logger.debug(f"Processing command: {self.user_input}")
-                    command_queue.put(self.user_input)
-                    self.update_cognitive_panel(f"User: {self.user_input}")
-                self.user_input = ""
-            elif ch in (curses.KEY_BACKSPACE, 127, 8):
-                self.user_input = self.user_input[:-1]
-            elif ch == 3:  # ctrl+c
-                logger.info("Received Ctrl+C, shutting down")
-                self.running = False
-            elif 32 <= ch <= 126:  # Printable characters
-                max_input = self.command_win.getmaxyx()[1] - 4
-                if len(self.user_input) < max_input:
-                    self.user_input += chr(ch)
-
-            self.update_command_panel(f"> {self.user_input}")
-            
-        except Exception as e:
-            logger.error(f"Error handling user input: {e}", exc_info=True)
-
-def start_visualization():
-    """Initialize and start the UI"""
-    logger.info("Starting visualization system")
+def start_monitor():
+    """Initialize and start the monitor"""
+    logger.info("Starting monitoring system")
     
     def run_wrapped(stdscr):
         try:
             curses.start_color()
             curses.use_default_colors()
-            curses.curs_set(0)
-            stdscr.nodelay(True)
+            curses.curs_set(0)  # Hide cursor
+            stdscr.nodelay(True)  # Non-blocking input
             
-            ui = TerminalUI(stdscr)
+            ui = MonitorUI(stdscr)
             ui.run()
             
         except Exception as e:
-            logger.critical(f"Fatal error in visualization: {e}", exc_info=True)
+            logger.critical(f"Fatal error in monitor: {e}", exc_info=True)
             raise
         finally:
-            logger.info("Visualization system shutdown")
+            logger.info("Monitor system shutdown")
     
     try:
         curses.wrapper(run_wrapped)
     except Exception as e:
-        logger.critical(f"Failed to start visualization: {e}", exc_info=True)
+        logger.critical(f"Failed to start monitor: {e}", exc_info=True)
         raise
 
 def handle_cognitive_event(event_data: str):
     """Queue cognitive event for display"""
     try:
-        logger.debug(f"Received cognitive event: {event_data[:50]}...")
         cognitive_queue.put(event_data)
     except Exception as e:
         logger.error(f"Error handling cognitive event: {e}", exc_info=True)
@@ -302,19 +343,6 @@ def handle_cognitive_event(event_data: str):
 def handle_state_event(event_data: str):
     """Queue state event for display"""
     try:
-        logger.debug(f"Received state event: {event_data[:50]}...")
         state_queue.put(event_data)
     except Exception as e:
-        logger.error(f"Error handling state event: {e}", exc_info=True)
-
-def poll_commands() -> Optional[str]:
-    """Poll for user commands"""
-    try:
-        cmd = command_queue.get_nowait()
-        logger.debug(f"Retrieved command: {cmd}")
-        return cmd
-    except Empty:
-        return None
-    except Exception as e:
-        logger.error(f"Error polling commands: {e}", exc_info=True)
-        return None
\ No newline at end of file
+        logger.error(f"Error handling state event: {e}", exc_info=True)
\ No newline at end of file
diff --git a/event_dispatcher.py b/event_dispatcher.py
index 03e0510..33f8cb2 100644
--- a/event_dispatcher.py
+++ b/event_dispatcher.py
@@ -35,8 +35,8 @@ class EventDispatcher:
         """
         self.listeners: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
         self.wildcard_listeners: List[Dict[str, Any]] = []
-        self.event_filter: Optional[str] = None # use one or the other to either choose an event to get rid of/view 
-        self.event_select: Optional[str] = None
+        self.event_filter: List[str] = ['timer', 'state', 'need', 'emotion'] # list of event types to filter out
+        self.event_select: List[str] = None # list of event types to select
 
     def add_listener(self, event_type: str, callback: Callable, priority: int = 0) -> None:
         """
@@ -81,11 +81,15 @@ class EventDispatcher:
             None, but prints error messages for exceptions in listeners.
         """
         # Check if event should be filtered
-        if self.event_filter and not event.event_type.startswith(self.event_filter + ':'):
-            EventLogger.log_event_dispatch(event.event_type, event.data, event.metadata)
-
-        if self.event_select and event.event_type.startswith(self.event_select + ':'):
-            EventLogger.log_event_dispatch(event.event_type, event.data, event.metadata)
+        # Log events that don't start with any filtered prefixes
+        if self.event_filter:
+            if not any(event.event_type.startswith(prefix + ':') for prefix in self.event_filter):
+                EventLogger.log_event_dispatch(event.event_type, event.data, event.metadata)
+
+        # Log events that start with any selected prefixes
+        if self.event_select:
+            if any(event.event_type.startswith(prefix + ':') for prefix in self.event_select):
+                EventLogger.log_event_dispatch(event.event_type, event.data, event.metadata)
 
         # Create a copy of the listeners for this event type
         listeners_to_call = self.listeners[event.event_type].copy()
diff --git a/internal/modules/cognition/cognitive_bridge.py b/internal/modules/cognition/cognitive_bridge.py
index 977de95..8c5f342 100644
--- a/internal/modules/cognition/cognitive_bridge.py
+++ b/internal/modules/cognition/cognitive_bridge.py
@@ -13,6 +13,8 @@ from internal.modules.memory.cognitive_memory_node import CognitiveMemoryNode
 
 import time
 
+from loggers.loggers import BrainLogger
+
 class CognitiveBridge:
     """
     Bridges conscious and subconscious processes, orchestrating memory traversal,
@@ -83,7 +85,7 @@ class CognitiveBridge:
         """
         try:
             # Initial search
-            matching_nodes = self.cognitive_memory.retrieve_memories(topic, self.internal_context.get_memory_context().raw_state, 5)
+            matching_nodes = self.cognitive_memory.retrieve_memories(topic, self.internal_context.get_memory_context(is_cognitive=True), 5)
             if not matching_nodes:
                 return []
 
@@ -146,11 +148,11 @@ class CognitiveBridge:
         """
         try:
             if not context_state:
-                context_state = self.internal_context.get_memory_context()
+                context_state = self.internal_context.get_memory_context(is_cognitive=True)
             # Get memories from cognitive system
             
             raw_state = context_state["raw_state"]
-            memories = self.cognitive_memory.retrieve_memories(query, raw_state, limit)
+            memories = self.cognitive_memory.retrieve_memories(query, context_state, limit)
             if not memories:
                 return []
 
@@ -227,6 +229,7 @@ class CognitiveBridge:
         Generate multi-system meditation effects.
         """
         try:
+            BrainLogger.info(f"Meditating on state: {state} ({intensity}, {duration})")
             intensity = max(0.0, min(1.0, intensity))
             duration = max(1, min(10, duration))
             effects = []
diff --git a/internal/modules/emotions/emotional_processor.py b/internal/modules/emotions/emotional_processor.py
index 4e90d02..b707854 100644
--- a/internal/modules/emotions/emotional_processor.py
+++ b/internal/modules/emotions/emotional_processor.py
@@ -25,6 +25,32 @@ class EmotionalVector:
         self.name = name
         self.timestamp = time.time()
 
+    def to_dict(self):
+        """Serializes EmotionalVector to dictionary."""
+        return {
+            'valence': self.valence,
+            'arousal': self.arousal,
+            'intensity': self.intensity,
+            'source_type': self.source_type,
+            'source_data': self.source_data,
+            'name': self.name,
+            'timestamp': self.timestamp
+        }
+
+    @classmethod
+    def from_dict(cls, data):
+        """Creates EmotionalVector from dictionary."""
+        vector = cls(
+            valence=data['valence'],
+            arousal=data['arousal'],
+            intensity=data['intensity'],
+            source_type=data.get('source_type'),
+            source_data=data.get('source_data'),
+            name=data.get('name')
+        )
+        vector.timestamp = data.get('timestamp', time.time())
+        return vector
+
     def __repr__(self):
         return (f"EmotionalVector(name={self.name}, valence={self.valence:.2f}, "
                 f"arousal={self.arousal:.2f}, intensity={self.intensity:.2f}, "
@@ -594,40 +620,46 @@ class EmotionalProcessor:
         if not echo_data or 'metadata' not in echo_data:
             return
             
-        # Extract core emotional data from echo
+        # Extract metadata and check for mood since emotional array is empty
         metadata = echo_data['metadata']
-        intensity = echo_data.get('intensity', 0.3)  # Default moderate intensity if not specified
-        
-        # Process the primary emotional state (first vector)
+        intensity = echo_data.get('intensity', 0.3)
+
+        # First try emotional data
         if 'emotional' in metadata and metadata['emotional']:
             primary_emotion = metadata['emotional'][0]
+            emotion_source = primary_emotion
+        # Fall back to mood data if available
+        elif 'mood' in metadata and metadata['mood'].get('mood'):
+            emotion_source = metadata['mood']['mood']
+        else:
+            return
             
-            # Create echo vector from primary emotion
-            echo_vector = EmotionalVector(
-                valence=primary_emotion['valence'],
-                arousal=primary_emotion['arousal'],
-                intensity=intensity * primary_emotion.get('intensity', 1.0),
-                source_type='memory_echo',
-                source_data={'echo_source': echo_data.get('source_node')},
-                name=primary_emotion.get('name', 'echo')
-            )
-            
-            # Apply echo-specific dampening
-            category = self._categorize_vector(echo_vector.valence, echo_vector.arousal)
-            echo_dampening = self._calculate_dampening(category) * 0.7  # Echo specific reduction
-            echo_vector.intensity *= echo_dampening
-            
-            # Add echo vector to current stimulus
-            self.current_stimulus.add_vector(echo_vector)
-            
-            # Log and dispatch the echo vector
-            global_event_dispatcher.dispatch_event_sync(Event("emotion:echo", {
-                "emotion": echo_vector,
-                "source": echo_data.get('source_node')
-            }))
-            
-            # Process influences on echo vector
-            self._process_influences(echo_vector)
+        # Create echo vector from emotion source
+        echo_vector = EmotionalVector(
+            valence=emotion_source['valence'],
+            arousal=emotion_source['arousal'], 
+            intensity=intensity * emotion_source.get('intensity', 0.25),
+            source_type='memory_echo',
+            source_data={'echo_source': echo_data.get('source_node')},
+            name=emotion_source.get('name', 'echo')
+        )
+        
+        # Apply echo-specific dampening
+        category = self._categorize_vector(echo_vector.valence, echo_vector.arousal)
+        echo_dampening = self._calculate_dampening(category) * 0.7  # Echo specific reduction
+        echo_vector.intensity *= echo_dampening
+        
+        # Add echo vector to current stimulus
+        self.current_stimulus.add_vector(echo_vector)
+        
+        # Log and dispatch the echo vector
+        global_event_dispatcher.dispatch_event_sync(Event("emotion:echo", {
+            "emotion": echo_vector,
+            "source": echo_data.get('source_node')
+        }))
+        
+        # Process influences on echo vector
+        self._process_influences(echo_vector)
 
     def process_meditation(self, event):
         """Processes meditation events for emotional influence."""
diff --git a/internal/modules/emotions/mood_synthesizer.py b/internal/modules/emotions/mood_synthesizer.py
index d3b0a23..1274b90 100644
--- a/internal/modules/emotions/mood_synthesizer.py
+++ b/internal/modules/emotions/mood_synthesizer.py
@@ -241,46 +241,44 @@ class MoodSynthesizer:
         echo_data = event.data
         if not echo_data or 'metadata' not in echo_data:
             return
-
-        metadata = echo_data['metadata']
-        if not metadata.get('emotional'):
-            return
-
-        # Get the primary emotional state from the echo
-        primary_emotion = metadata['emotional'][0]
-
-        # Scale echo influence by the echo intensity and mood system weights
-        echo_intensity = echo_data.get('intensity', 0.3) * 0.4  # Reduced influence vs direct emotions
-        echo_valence = primary_emotion['valence'] * echo_intensity
-        echo_arousal = primary_emotion['arousal'] * echo_intensity
-
-        # Create temporary mood nudge from echo
-        echo_mood = Mood(
-            valence=self.current_mood.valence + (echo_valence * self.weights['emotions']),
-            arousal=self.current_mood.arousal + (echo_arousal * self.weights['emotions'])  
-        )
-
-        # Update mood if the echo influence is significant
-        if abs(echo_mood.valence - self.current_mood.valence) > 0.1 or \
-           abs(echo_mood.arousal - self.current_mood.arousal) > 0.1:
-            
-            # Clamp values
-            echo_mood.valence = max(-1.0, min(1.0, echo_mood.valence))
-            echo_mood.arousal = max(-1.0, min(1.0, echo_mood.arousal))
             
-            # Apply the echo-influenced mood
-            old_name = self.current_mood_name
-            self.current_mood = echo_mood
-            new_name = self._map_mood_to_name(echo_mood)
-            
-            if new_name != old_name:
-                self.current_mood_name = new_name
-                global_event_dispatcher.dispatch_event_sync(Event("mood:changed", {
-                    "old_name": old_name,
-                    "new_name": new_name,
-                    "mood_object": echo_mood,
-                    "source": "memory_echo"
-                }))
+        metadata = echo_data['metadata']
+        
+        # Extract mood influence from metadata if available
+        mood_data = metadata.get('mood', {}).get('mood', {})
+        if mood_data and 'valence' in mood_data and 'arousal' in mood_data:
+            # Scale echo influence by the echo intensity 
+            echo_intensity = echo_data.get('intensity', 0.3) * 0.4  # Reduced influence
+            echo_valence = mood_data['valence'] * echo_intensity  
+            echo_arousal = mood_data['arousal'] * echo_intensity
+
+            # Create temporary mood nudge combining current mood and echo
+            echo_mood = Mood(
+                valence=self.current_mood.valence + (echo_valence * self.weights['emotions']),
+                arousal=self.current_mood.arousal + (echo_arousal * self.weights['emotions'])
+            )
+
+            # Update mood if the echo influence is significant
+            if abs(echo_mood.valence - self.current_mood.valence) > 0.1 or \
+               abs(echo_mood.arousal - self.current_mood.arousal) > 0.1:
+                
+                # Clamp values
+                echo_mood.valence = max(-1.0, min(1.0, echo_mood.valence))
+                echo_mood.arousal = max(-1.0, min(1.0, echo_mood.arousal))
+                
+                # Apply the echo-influenced mood
+                old_name = self.current_mood_name
+                self.current_mood = echo_mood
+                new_name = self._map_mood_to_name(echo_mood)
+                
+                if new_name != old_name:
+                    self.current_mood_name = new_name
+                    global_event_dispatcher.dispatch_event_sync(Event("mood:changed", {
+                        "old_name": old_name,
+                        "new_name": new_name,
+                        "mood_object": echo_mood,
+                        "source": "memory_echo"
+                    }))
 
     def process_meditation(self, event):
         """
diff --git a/internal/modules/memory/cognitive_memory.py b/internal/modules/memory/cognitive_memory.py
index efd9f64..3694275 100644
--- a/internal/modules/memory/cognitive_memory.py
+++ b/internal/modules/memory/cognitive_memory.py
@@ -353,7 +353,7 @@ class CognitiveMemory:
 
         Args:
             query: The query string to retrieve relevant memories
-            given_state: The given internal state for comparison
+            given_state: The given internal state for comparison (from is_cognitive, raw_state, processed_state)
             top_k: Number of top memories to retrieve
             return_details: If True, returns detailed retrieval metrics alongside nodes
             
@@ -463,6 +463,9 @@ class CognitiveMemory:
             metrics['emotional'] = emotional_metrics
             
             # 3. State Analysis (broken down by component)
+            print("Debug - target_node.raw_state: ", target_node.raw_state)
+            print("Debug - target_node.processed_state: ", target_node.processed_state)
+            print("Debug - comparison_state: ", comparison_state)
             state_metrics = self._calculate_state_metrics(
                 target_node.raw_state,
                 target_node.processed_state,
@@ -1801,11 +1804,39 @@ class CognitiveMemory:
         Orchestrate echo activation and handle strength adjustments based on 
         comprehensive evaluation metrics.
         """
-        if not comparison_state:
+        # Validate comparison state structure
+        valid_comparison_state = True
+        if comparison_state:
+            # Check for required structure
+            if not ('raw_state' in comparison_state and 'processed_state' in comparison_state):
+                self.logger.warning(
+                    "Invalid comparison state structure received in trigger_echo. "
+                    f"State: {comparison_state}, Query: {query_text}"
+                )
+                valid_comparison_state = False
+
+        # Get current context if no valid comparison state
+        if not comparison_state or not valid_comparison_state:
             comparison_state = self.internal_context.get_memory_context(is_cognitive=True)
+            self.logger.warning(f"Replacing with {comparison_state} in trigger_echo")
+            if not comparison_state:
+                self.logger.error("Failed to get cognitive memory context in trigger_echo")
+                return
+
+        # Extract query text from processed state if not provided
         if not query_text:
-            query_text = comparison_state['processed_state']['cognitive']
-        if not query_embedding:
+            # Safely try to get from processed state cognitive field
+            try:
+                query_text = comparison_state.get('processed_state', {}).get('cognitive', '')
+                if not query_text:
+                    self.logger.warning("No query text found in processed state")
+                    # Could set a default here if needed
+            except Exception as e:
+                self.logger.warning(f"Error extracting query text from state: {e}")
+                query_text = ''  # Set empty default
+
+        # Generate embedding if needed
+        if not query_embedding and query_text:
             query_embedding = self._generate_embedding(query_text)
             
         selected_node, final_intensity, details = self.evaluate_echo(node, comparison_state, query_text, query_embedding)
diff --git a/internal/modules/needs/needs_manager.py b/internal/modules/needs/needs_manager.py
index bdecb81..e76a392 100644
--- a/internal/modules/needs/needs_manager.py
+++ b/internal/modules/needs/needs_manager.py
@@ -70,9 +70,8 @@ class NeedsManager:
         for need_name in ['boredom', 'loneliness']:
             if need_name in remembered_needs:
                 current = self.needs[need_name].value
-                # Try to get either current_value or value
-                remembered = (remembered_needs[need_name].get('current_value') or 
-                            remembered_needs[need_name].get('value', current))
+                # Get value from the remembered needs structure
+                remembered = remembered_needs[need_name].get('value', current)
                 
                 # Calculate moderate shift toward remembered state
                 shift = (remembered - current) * echo_data.get('intensity', 0.3) * 0.4
diff --git a/loggers/loggers.py b/loggers/loggers.py
index 32a9744..397929e 100644
--- a/loggers/loggers.py
+++ b/loggers/loggers.py
@@ -260,8 +260,15 @@ class EventLogger:
         components = [f"Event Dispatched: {event_type}"]
         
         if data is not None:
-            if isinstance(data, (dict, list)):
-                data_str = json.dumps(data, indent=2)
+            if isinstance(data, dict):
+                # Convert any EmotionalVector objects to dictionaries
+                processed_data = {k: v.to_dict() if hasattr(v, 'to_dict') else v for k, v in data.items()}
+                data_str = json.dumps(processed_data, indent=2)
+                components.append(f"Data:\n{data_str}")
+            elif isinstance(data, list):
+                # Convert any EmotionalVector objects in list
+                processed_data = [item.to_dict() if hasattr(item, 'to_dict') else item for item in data]
+                data_str = json.dumps(processed_data, indent=2)
                 components.append(f"Data:\n{data_str}")
             else:
                 components.append(f"Data: {str(data)}")
diff --git a/main.py b/main.py
index 8ecf0db..efb40ef 100644
--- a/main.py
+++ b/main.py
@@ -15,7 +15,7 @@ from core.server import HephiaServer
 from config import Config, ProviderType
 from loggers import LogManager
 from event_dispatcher import global_event_dispatcher
-from display.hephia_tui import start_visualization, handle_cognitive_event, handle_state_event, poll_commands
+from display.hephia_tui import handle_cognitive_event, handle_state_event, start_monitor
 
 LogManager.setup_logging()
 
@@ -85,46 +85,21 @@ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
         server = HephiaServer()
 
         # Start interface in a separate thread
-        vis_thread = threading.Thread(target=start_visualization, daemon=True)
+        vis_thread = threading.Thread(target=start_monitor, daemon=True)
         vis_thread.start()
 
         # Hook up event handlers
         # Cognitive context handler
         global_event_dispatcher.add_listener(
             "cognitive:context_update",
-            lambda event: handle_cognitive_event(
-                f"Cognitive State:\n"
-                f"Summary: {event.data.get('processed_state', 'No summary available')}\n"
-                f"Recent Messages:\n" + 
-                "\n".join(
-                    f"  {msg['role']}: {msg['content'][:100]}..." 
-                    for msg in event.data.get('raw_state', [])[-3:]
-                )
-            )
+            lambda event: handle_cognitive_event(event)
         )
 
         # System state handler 
         global_event_dispatcher.add_listener(
             "state:changed",
-            lambda event: handle_state_event(
-                f"System State:\n"
-                f"Mood: {event.data.get('context', {}).get('mood', {}).get('name', 'unknown')} "
-                f"(v:{event.data.get('context', {}).get('mood', {}).get('valence', 0):.2f}, "
-                f"a:{event.data.get('context', {}).get('mood', {}).get('arousal', 0):.2f})\n"
-                f"Behavior: {event.data.get('context', {}).get('behavior', {}).get('name', 'none')}\n"
-                f"Needs: {', '.join(f'{k}: {str(v)}' for k,v in event.data.get('context', {}).get('needs', {}).items())}\n"
-                f"Emotional State: {event.data.get('context', {}).get('emotional_state', 'neutral')}"
-            )
+            lambda event: handle_state_event(event)
         )
-
-        # Poll for commands periodically
-        async def command_polling():
-            while True:
-                poll_commands()
-                await asyncio.sleep(0.5)
-
-        # Add command polling to your server tasks
-        asyncio.create_task(command_polling())
         
         print("""
 Hephia is now active! 
